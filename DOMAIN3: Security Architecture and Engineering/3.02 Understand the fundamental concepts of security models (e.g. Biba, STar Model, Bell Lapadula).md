## 3.2 Understand the fundamental concepts of security models (e.g. Biba, STar Model, Bell Lapadula) ##

The **Trusted Computing Base (TCB)** refers to the core components of a system that are responsible for enforcing security. It includes:
- The Operating System Kernel
- Security mechanisms (e.g., access control, authentication)
- Hardware components that support security

If the TCB is compromised, the entire system is at risk, which is why it must be carefully designed and protected.

```mermaid
stateDiagram-v2
    [*] --> UserRequest

    UserRequest --> TCB_ChecksAccess : Request access to resource
    TCB_ChecksAccess --> AccessGranted : Policy allows it
    TCB_ChecksAccess --> AccessDenied : Policy denies it

    note right of TCB_ChecksAccess
      The TCB evaluates all access requests
      using mandatory security policies.
      It must be protected from tampering.
    end note

    classDef center fill:#ffffff,color:#000000,stroke:#000000,stroke-width:1px,font-weight:bold
    classDef node fill:#e0e0e0,color:#000000,stroke:#000000,stroke-width:1px
```

The **security perimeter** defines the boundary between trusted and untrusted areas of a system. It includes firewalls, encryption, access controls, and authentication mechanisms that protect internal systems from external threats.
For example a company's internal network (trusted) is separated from the internet (untrusted) by a firewall and VPN gateways, ensuring only authorized users can access internal resources.

---

**Security composition theories** help determine whether combining multiple secure systems results in an overall secure system. There are 3 composition theories:

- Cascading → System A feeds into System B. If A and B are secure, the combination should be secure.
- Feedback → System A and B influence each other. If not properly managed, security issues can arise.
- Hook-up → System A and B interact with a third system. It requires additional security evaluation.

**Cascading Composition:**
```mermaid
stateDiagram-v2
    [*] --> SystemA
    SystemA --> SystemB : Output of A → Input of B
    SystemB --> FinalOutput

    note right of SystemA
      System A processes input securely
    end note

    note right of SystemB
      System B must handle As output securely
    end note

    classDef center fill:#ffffff,color:#000000,stroke:#000000,stroke-width:1px,font-weight:bold
    classDef node fill:#e0e0e0,color:#000000,stroke:#000000,stroke-width:1px
```
**Feedback Composition:**
```mermaid
stateDiagram-v2
    [*] --> SystemA
    SystemA --> SystemB : Output loops back as new input
    SystemB --> SystemA : Output loops back as new input
    SystemB --> FinalOutput

    note right of SystemA
      Security must be enforced continuously
      as output affects future behavior
    end note

    classDef center fill:#ffffff,color:#000000,stroke:#000000,stroke-width:1px,font-weight:bold
    classDef node fill:#e0e0e0,color:#000000,stroke:#000000,stroke-width:1px
```

**Hook-up composition:**
```mermaid
stateDiagram-v2
    [*] --> SystemA
    [*] --> SystemB
    SystemA --> SharedEnvironment
    SystemB --> SharedEnvironment
    SharedEnvironment --> FinalOutput

    note right of SharedEnvironment
      Interaction point must preserve
      confidentiality, integrity, and isolation
    end note


    classDef center fill:#ffffff,color:#000000,stroke:#000000,stroke-width:1px,font-weight:bold
    classDef node fill:#e0e0e0,color:#000000,stroke:#000000,stroke-width:1px
```

---
**Security models** provide structured frameworks to enforce security policies and ensure that systems operate securely. These models define rules for access control, information flow, and system security. 

A **state machine** model ensures that a system is always in a secure state. Think of a state as a snapshot of a system at a specific moment, including user permissions, data access levels, and active processes.
For a system to be considered secure, every transition between states (such as logging in, opening a file, or executing a program) must maintain security properties. This means that if a system starts in a secure state and follows the defined rules, it will always remain secure.

These are the main components of a state machine model:
| Component                 | Description                                                                   |
|--------------------------|-------------------------------------------------------------------------------|
| **States**                | The various conditions or modes a system can be in (e.g., logged in, locked). |
| **Events/Input**          | Triggers that cause state transitions (e.g., a login attempt).                |
| **Transitions**           | Rules that move the system from one state to another.                         |
| **Initial State**         | The default starting point (e.g., system off or user logged out).             |
| **Final/Accepting State** | End state or condition where a process completes.                             |

In the following you can find a login system which is an example of a state machine model:

```mermaid
stateDiagram-v2
    [*] --> LoggedOut
    LoggedOut --> LoggingIn: User enters credentials
    LoggingIn --> LoggedIn: Credentials valid
    LoggingIn --> LoggedOut: Credentials invalid
    LoggedIn --> LoggedOut: User logs out

    classDef center fill:#ffffff,color:#000000,stroke:#000000,stroke-width:1px,font-weight:bold
    classDef node fill:#e0e0e0,color:#000000,stroke:#000000,stroke-width:1px
```

The **lattice** model is used in mandatory access control (MAC) systems, where data and users are assigned security labels. These labels determine who can access what, using a mathematical structure called a lattice.
The main idea is simple:

- Every user and object (files, databases, etc.) has a classification level (e.g., Top Secret, Secret, Confidential, Unclassified).

- Users can only access information at their level or lower (e.g., a Secret user can access Confidential data but not Top Secret data).

For example, in military organizations, a general with Top Secret clearance can access all classified information, while a soldier with Confidential clearance cannot see Secret or Top Secret data.

These are the main components of a state machine model:
| Component             | Description                                                                 |
|----------------------|-----------------------------------------------------------------------------|
| **Subjects**          | Active entities (e.g., users, processes) requesting access to objects.     |
| **Objects**           | Passive entities (e.g., files, databases) being accessed.                  |
| **Security Labels**   | Labels (e.g., Top Secret, Secret, Confidential) assigned to subjects/objects. |
| **Security Lattice**  | A structure defining the dominance relationship among labels.              |
| **Dominance Relation**| A rule that defines when a subject can access an object based on labels.   |
| **Access Rules**      | Determines if a subject can read/write an object based on the lattice.     |


```mermaid
stateDiagram-v2
    [*] --> RequestAccess: Subject requests access to Object
    RequestAccess --> CheckLabels: Compare subject and object labels
    CheckLabels --> AccessGranted: If subject dominates object (read) or object dominates subject (write)
    CheckLabels --> AccessDenied: If dominance rule is not satisfied


    classDef center fill:#ffffff,color:#000000,stroke:#000000,stroke-width:1px,font-weight:bold
    classDef node fill:#e0e0e0,color:#000000,stroke:#000000,stroke-width:1px
```

**Information Flow** Models ensure that data flows in a secure manner, preventing leaks or unauthorized access. They focus on how information moves within a system rather than just access permissions.

Bell-LaPadula Model (Confidentiality Focused) → Prevents unauthorized reading of higher-classified data (No Read Up) and unauthorized writing to lower-classified data (No Write Down).

Biba Model (Integrity Focused) → Prevents users from modifying higher-integrity data (No Write Up) and from reading lower-integrity data (No Read Down).

For example e financial system using the Biba Model would prevent an intern from modifying critical accounting records, ensuring only senior accountants can make changes.

These are the main components of an information flow model:
| Component             | Description                                                                 |
|----------------------|-----------------------------------------------------------------------------|
| **Subjects**          | Active entities (e.g., users, programs) that send or receive information.  |
| **Objects**           | Passive entities (e.g., files, databases) involved in information exchange. |
| **Information Flow**  | The movement or transfer of data between subjects and/or objects.           |
| **Flow Policy**       | Rules that determine allowed and disallowed data flows based on labels or levels. |
| **Security Labels**   | Classifications assigned to subjects and objects (e.g., Confidential, Public). |
| **Channels**          | Logical paths or mechanisms used to transfer data.                          |
| **Direction of Flow** | Indicates whether flow is read, write, or communication between components. |

```mermaid
stateDiagram-v2
    [*] --> FlowRequest: Subject attempts to send or receive information
    FlowRequest --> CheckPolicy: System checks flow against security policy
    CheckPolicy --> AllowedFlow: Flow is permitted under current policy
    CheckPolicy --> BlockedFlow: Flow is denied to prevent a policy violation

    classDef center fill:#ffffff,color:#000000,stroke:#000000,stroke-width:1px,font-weight:bold
    classDef node fill:#e0e0e0,color:#000000,stroke:#000000,stroke-width:1px
```

A **noninterference model** ensures that high-level users (e.g., admins, military officers) do not influence low-level users (e.g., general employees, soldiers) in a way that reveals sensitive data. The idea is that actions taken by high-privileged users should not affect what lower-privileged users see or do. This model is often used to prevent covert channels (hidden ways for users to communicate unauthorized information).

For example In a classified government database, if an admin searches for “Top Secret Spy List,” lower-level users should not notice changes in system response time (which might hint at the existence of such a list).

These are the main components of a non-interference model:
| Component             | Description                                                                 |
|----------------------|-----------------------------------------------------------------------------|
| **Subjects**          | Active entities (e.g., users, processes) interacting with the system.       |
| **High-Level Subjects (H)** | Subjects with access to sensitive or confidential data.                     |
| **Low-Level Subjects (L)** | Subjects with limited or no access to sensitive data.                        |
| **System States**     | Represent the observable behavior of the system.                             |
| **Actions**           | Operations performed by subjects (e.g., read, write, execute).               |
| **Non-Interference**  | Property stating that actions of high-level subjects must not affect what low-level subjects can observe. |
| **Observables**       | The outputs or state changes that low-level users can perceive.              |

```mermaid
stateDiagram-v2
    [*] --> L_Subject_Performs_Action
    L_Subject_Performs_Action --> System_State_L
    System_State_L --> L_Observes_State

    [*] --> H_Subject_Performs_Action
    H_Subject_Performs_Action --> System_State_H

    note right of System_State_H
      High-level action changes system state,
      but low-level subjects cannot observe it.
    end note

    note right of L_Observes_State
      Low-level subject only sees effects
      of low-level actions.
      High-level actions are invisible.
    end note


    classDef center fill:#ffffff,color:#000000,stroke:#000000,stroke-width:1px,font-weight:bold
    classDef node fill:#e0e0e0,color:#000000,stroke:#000000,stroke-width:1px
```

---

The **Bell–LaPadula Model (BLP)** is a mandatory access control (MAC) model designed to enforce confidentiality in secure systems. Developed in 1973 for military and government use, it prevents unauthorized access to classified data by restricting read and write operations based on security levels. 

Key Concepts of Bell–LaPadula:

- Goal: Protect confidentiality by ensuring users can only access data at or below their clearance level.
- Security Levels: Data and users are classified hierarchically (e.g., Top Secret, Secret, Confidential, Unclassified).
- Access Control: Uses Mandatory Access Control (MAC) and incorporates Discretionary Access Control (DAC) through the Discretionary Security Property.

The core Security rules of the model are:

1️. No Read Up (Simple Security Property): Users cannot read data at a higher security level than their own. Prevents unauthorized users from accessing classified information. Example: A Secret-level analyst cannot read a Top Secret file.

2️. No Write Down (Star Property, -Property): Users cannot write to a lower security level. Prevents leaks of sensitive information by blocking downgrades. Example: A Top Secret officer cannot save a classified report in an Unclassified document.

3️. Discretionary Security Property (ds-property): Enforces additional access restrictions based on permissions set by the data owner. Even if a user has the right security level, they must also be granted access. Example: A Secret user cannot access a Secret document unless they have explicit permission.

Strengths  of the model:
- Enforces strict confidentiality (ideal for government/military).
- Prevents unauthorized access and leaks of classified data.
- Combines MAC and DAC for better security.


Limitations of the model:
- Does not protect data integrity (modifications are not controlled).
- Not suitable for commercial environments requiring flexible access.
- Vulnerable to covert channels, where users infer information indirectly.

```mermaid

stateDiagram-v2
    direction TB

    state "User (Secret Level)" as User
    state "File A (Top Secret)" as FileTS
    state "File B (Secret)" as FileS
    state "File C (Confidential)" as FileC

    User --> FileTS : ❌ Read (No Read Up)
    User --> FileS : ✅ Read ✅ Write
    User --> FileC : ❌ Write (No Write Down)

    note right of FileTS
      Higher level Can't read (No Read Up)
    end note

    note right of FileC
      Lower level Can't write (No Write Down)
    end note

    note left of User
      "Secret-level" user wants to access data
    end note


    classDef center fill:#ffffff,color:#000000,stroke:#000000,stroke-width:1px,font-weight:bold
    classDef node fill:#e0e0e0,color:#000000,stroke:#000000,stroke-width:1px
```

:brain: The Bell-LaPadula Model focuses on maintaining confidentiality by enforcing two (+1 ds-property) key rules: the "no read up" (a subject cannot read data at a higher security level) and the "no write down" (a subject cannot write data to a lower security level). It ensures data confidentiality by restricting access based on security clearances.


The **Biba Model** is a mandatory access control (MAC) model designed to enforce data integrity. Developed in 1977 by Kenneth Biba, it prevents unauthorized modifications to data by restricting read and write operations based on integrity levels. Unlike the Bell–LaPadula Model, which focuses on confidentiality, the Biba Model ensures that data remains accurate and unaltered. 

Key Concepts of the Biba Model

- Goal: Protect data integrity by ensuring users can only modify data at or above their integrity level.
- Integrity Levels: Data and users are classified hierarchically (e.g., High, Medium, Low).
- Access Control: Uses Mandatory Access Control (MAC) to restrict data modification.

The Core Security Rules of the Biba Model are:

1. No Write Up (Simple Integrity Property)

  Users cannot write data at a higher integrity level than their own.

  Prevents untrusted users from modifying critical data.

2. No Read Down (Star Integrity Property, -Property)

  Users cannot read data at a lower integrity level.

  Prevents contamination of high-integrity data by unreliable sources.

Strengths of the model:

- Ensures data integrity, making it ideal for financial and critical systems.
- Prevents unauthorized modifications and data corruption.
- Protects against malware by restricting low-integrity processes.

Limitations of the model:

- Does not address confidentiality (focuses only on integrity).
- Not suitable for systems where users need to access and modify lower-level data.
- Can be restrictive in dynamic environments where data needs to flow between different levels.

```mermaid
stateDiagram-v2
    direction TB

    state "User (Medium Integrity)" as User
    state "File A (High)" as FileHigh
    state "File B (Medium)" as FileMed
    state "File C (Low)" as FileLow

    User --> FileHigh : ❌ Write (No Write Up)
    User --> FileMed : ✅ Read/Write
    User --> FileLow : ❌ Read (No Read Down)

    note right of FileHigh
      Higher level Can't write (No Write Up)
    end note

    note right of FileLow
      Lower level Can't read (No Read Down)
    end note

    note left of User
      "Medium-level" user wants to preserve integrity
    end note

   classDef center fill:#ffffff,color:#000000,stroke:#000000,stroke-width:1px,font-weight:bold
    classDef node fill:#e0e0e0,color:#000000,stroke:#000000,stroke-width:1px
```


:brain: The Biba Model focuses on maintaining data integrity by enforcing two key rules: the "no write up" (a subject cannot write to a higher security level) and the "no read down" (a subject cannot read data at a lower security level). It ensures that data is not corrupted by unauthorized modifications.


The **Take-Grant Model** is an access control model used to analyze and enforce access rights in a system. It describes how rights can be transferred between subjects (users, processes) and objects (files, resources) within a system. This model is useful for understanding how permissions can propagate and whether unauthorized access can occur.

The Key Concepts of the Take-Grant Model are:

- Goal: Model and analyze how permissions spread between subjects and objects.
- Graph Representation: The system is represented as a directed graph, where:
    - Nodes represent subjects (users, processes) and objects (files, resources).
    - Edges (arrows) represent the rights that subjects have over objects.
- Access Rights: Can include read, write, execute, delete, or grant permissions.
- Two Main Operations: Take and Grant determine how permissions are transferred.

The Core Rules of the Take-Grant Model are:

1. Take Rule: A subject with take rights over another subject or object can acquire any rights from it.
2. Grant Rule: A subject with grant rights can give its access rights to another subject.
3. Create Rule: A subject can create new objects and assign permissions to them.
4. Remove Rule: A subject can remove rights it previously granted.

Strengths of the Take-Grant Model:

- Models access rights propagation clearly using a graph-based approach.
- Helps predict security risks by analyzing how permissions can spread.
- Efficient and simple to implement in many access control scenarios.

Limitations of the Take-Grant Model:

- Does not enforce access control policies, only analyzes them.
- Assumes all rights are transferable, which is not always the case in real-world systems.
- Not designed for mandatory access control (MAC) systems, where permissions are strictly defined by policies.

```mermaid
flowchart TB

subgraph Take_Rule
    direction LR
    S1_Take1[["S1"]] -->|t| O1_Take1[["O1"]] -->|r| O2_Take1[["O2"]]
    S1_Take2[["S1"]] -->|r| O1_Take2[["O1"]] -->|r| O2_Take2[["O2"]]
    S1_Take2[["S1"]] ==>|r| O2_Take2[["O2"]]
    label1[Take Rule  - Before]:::label -.- S1_Take1
    label2[Take Rule - After]:::label -.- S1_Take2
end

subgraph Grant_Rule
    direction LR
    S1_Grant1[["S1"]] -->|g| O1_Grant1[["O1"]]
    S1_Grant1[["S1"]] -->|r| O2_Grant1[["O2"]]
    S1_Grant2[["S1"]] -->|g| O1_Grant2[["O1"]] ==>|r| O2_Grant2[["O2"]]
    S1_Grant2[["S1"]] -->|r| O2_Grant2[["O2"]]
    label3[Grant Rule  - Before]:::label -.- S1_Grant1
    label4[Grant Rule - After]:::label -.- S1_Grant2
end

    classDef center fill:#ffffff,color:#000000,stroke:#000000,stroke-width:1px,font-weight:bold
    classDef node fill:#e0e0e0,color:#000000,stroke:#000000,stroke-width:1px

```

:brain: The Take-Grant Model is a permission-based access control model where subjects (users or processes) can take or grant access to objects (files, resources) based on their permissions. The model defines a set of rules that determine how permissions are transferred between subjects and objects, ensuring controlled access and authority delegation.


The **Clark-Wilson Model** is a security model designed to enforce data integrity in systems where accuracy, consistency, and authorized modifications are critical. Unlike models such as Bell–LaPadula (which focuses on confidentiality) or Biba (which focuses on integrity at a broad level), the Clark-Wilson Model ensures that only authorized users can make valid changes to data through well-defined processes. This makes it particularly useful in commercial systems, banking, and financial transactions.

Key Concepts of the Clark-Wilson Model are:

- Goal: Ensure data integrity by enforcing rules that prevent unauthorized changes.
- Separation of Duties: Users cannot modify critical data directly; instead, transactions must be approved and validated before execution.
- Transformation Procedures (TPs): Authorized processes that change data in a controlled way.
- Constrained Data Items (CDIs): Critical data that must be protected (e.g., bank records, financial transactions).
- Unconstrained Data Items (UDIs): Regular data that does not require strict controls.
- Integrity Verification Procedures (IVPs): Periodic checks that ensure data integrity is maintained.

Core Rules of the Clark-Wilson Model
1. Well-Formed Transactions: Users cannot modify critical data directly; all changes must go through authorized procedures (TPs).
2. Separation of Duties: Users cannot perform both critical functions (e.g., initiating and approving a transaction).
3. Access Control & Authentication: Only authorized users can execute Transformation Procedures (TPs) on Constrained Data Items (CDIs).
4. Integrity Verification Procedures (IVPs): The system must periodically verify data integrity and ensure that no unauthorized modifications have occurred.

Strengths of the Clark-Wilson model:

- Prevents unauthorized modifications by requiring approved transactions.
- Reduces fraud and insider threats by enforcing Separation of Duties.
- Ensures consistency and reliability, making it ideal for financial and enterprise systems.

Limitations of the Clark-Wilson model:
- More complex than other models due to its focus on transaction-based security.
- Requires strict enforcement of Transformation Procedures, which can slow down operations.
- Not designed for confidentiality, making it less useful for classified or government data protection.

```mermaid

flowchart TD
    subgraph "Certification - After"
        CW1a[User]
        CW2a[Certified T.P.]
        CW3a[CDI: Customer Data]
        CW1a -->|invoke| CW2a
        CW2a -->|read/write| CW3a
    end


    subgraph "Certification - Before"
        CW1[User]
        CW2[Uncertified T.P.]
        CW3[CDI: Customer Data]
        CW1 --> CW2
        CW2 --> CW3
    end

    classDef center fill:#ffffff,color:#000000,stroke:#000000,stroke-width:1px,font-weight:bold
    classDef node fill:#e0e0e0,color:#000000,stroke:#000000,stroke-width:1px

```

The *Brewer and Nash Model* is a conflict-of-interest security model designed to prevent users from accessing data that could create an ethical conflict. It ensures that a user cannot access sensitive information from competing organizations, preventing insider trading, financial conflicts, or biased decision-making.

Key Concepts of the Brewer and Nash Model are:
- Goal: Prevent conflicts of interest by restricting users from accessing data from competing organizations.
- Dynamic Access Control: A user’s past access history determines what they can access in the future.
- Company Dataset (CDS): A collection of files and data related to a single company.
- Conflict of Interest Class (COI): A group of companies that compete with each other (e.g., different banks, law firms, or consulting clients).
- Access Restriction: Once a user accesses one company’s data, they are blocked from accessing data from competitors within the same Conflict of Interest Class

Core Rules of the Brewer and Nash Model are:
1. Read Access Rule: A user can only read from one company within a Conflict of Interest Class (COI).
2. Write Access Rule: A user can only write to a dataset if they have not read any conflicting dataset.
3. Dynamic Security Enforcement: The model updates restrictions dynamically based on the user’s previous access history.

Strengths of the Brewer and Nash Model:

- Prevents conflicts of interest, making it ideal for law firms, auditors, and financial analysts.
- Dynamically adapts based on a user’s access history, providing strong security.
- Ensures ethical business practices by restricting access to sensitive competitor data.

Limitations of the Brewer and Nash Model:

- Not suitable for traditional access control since it focuses on conflicts of interest rather than general security.
- Hard to implement in large organizations with complex cross-business interactions.
- May restrict legitimate work if employees need to collaborate across different companies.

```mermaid
flowchart TD
    A[User requests access to Company A data] --> B{Has user accessed<br> competing company data?}
    B -- No --> C[Access granted to Company A]
    C --> D[User now associated<br> with Company A's conflict class]
    B -- Yes --> E[Access denied to Company A]

    classDef center fill:#ffffff,color:#000000,stroke:#000000,stroke-width:1px,font-weight:bold
    classDef node fill:#e0e0e0,color:#000000,stroke:#000000,stroke-width:1px

```

The **Graham-Denning Model** is an access control model that defines how subjects (users, processes) can securely create, delete, and manage access rights for objects (files, resources) in a system. It provides a set of rules that regulate how rights are assigned and removed, making it useful for operating systems, databases, and security policies that require structured access control mechanisms.

The Key Concepts of the Graham-Denning Model are:

- Goal: Control how subjects create, delete, and modify other subjects, objects, and their associated rights.
- Access Control Matrix (ACM): A table that tracks who has what permissions over different objects.
- Subjects: Users or processes that request access to objects.
- Objects: Files, databases, or system resources that require protection.
- Rights: Permissions such as read, write, execute, own, grant.

The model defines eight fundamental operations that manage subjects, objects, and their access rights.

Operations on Subjects and Objects
1. Create Object – A subject can create a new object (e.g., a file) and becomes its owner.
2. Create Subject – A subject can create another subject (e.g., a new user or process).
3. Delete Object – A subject can delete an object, removing all associated rights.
4. Delete Subject – A subject can delete another subject, revoking its permissions.

Operations on Access Rights
1. Read Access Rights – A subject can view the access rights assigned to an object.
2. Grant Access Rights – A subject can grant its own permissions to another subject.
3. Delete Access Rights – A subject can revoke previously assigned permissions.
4. Transfer Access Rights – A subject can pass its access rights to another subject, who then assumes control.

Strengths of the Graham-Denning model:
- Provides structured rules for managing access rights.
- Supports dynamic access control, allowing permissions to be updated as needed.
- Useful in multi-user environments, such as operating systems, cloud services, and databases.

Limitations of the Graham-Denning model:
- Does not enforce security policies on its own—it provides a framework, but implementation details must be defined.
- Focuses on access control, without addressing integrity or confidentiality like Bell–LaPadula or Biba.
- Can be complex when managing large-scale systems with many subjects and objects.

```mermaid
flowchart TD
%% Subjects
    SA[Subject A: Admin]
    SB[Subject B: Process/User]

%% Objects
    OF[Object File.txt]
    OD[Object DB-Record]

%% Access Control Matrix
    ACM[[Access Control Matrix]]

%% Operations on Subjects & Objects
    SA -->|Create Subject| SB
    SA -->|Create Object| OF
    SA -->|Delete Object| OD
    SA -->|Delete Subject| SB

%% Operations on Rights
    SA -->|Read Access Rights| ACM
    SA -->|Grant Read| SB
    SA -->|Delete Access Rights| SB
    SA -->|Transfer Ownership| SB
    

    classDef center fill:#ffffff,color:#000000,stroke:#000000,stroke-width:1px,font-weight:bold
    classDef node fill:#e0e0e0,color:#000000,stroke:#000000,stroke-width:1px

```

